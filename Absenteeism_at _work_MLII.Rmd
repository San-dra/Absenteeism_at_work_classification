---
title: "Predicting Absenteeism at work"
subtitle: "Machine Learning II"
author: "Sandra Alemayehu & Jessica Hayek"
date: "21 February 2020"
output:
  rmarkdown::html_document:
    code_folding: show
    df_print: paged
    fig_caption: true
    theme: lumen
    toc: yes
    toc_depth: 4
    number_sections: true
    toc_float: yes
---
```{r Setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Packages Needed, message=FALSE, warning=FALSE, include=FALSE}
library(prettydoc) #For rmd theme
library(MASS)
library(ISLR)
library(rpart)
library(rpart.plot)
library(randomForest)
library(ggplot2)
library(e1071)     
library(glmnet)
library(caret)     
library(FSelector)
library(dplyr)
library(knitr)
library(magrittr)
library(lattice)
library(skimr)
library('binr')
library(gridExtra)
library(devtools)

```

# Introduction
The main goal is to **predict whether or not an individual is going to have a long absence** (`Absenteeism` variable) it's therefore a binary classification problem.

# Data Loading and Overview


```{r Loading Training}

abs_trainset_df<-read.csv("C:/Users/sandr/Desktop/Machine Learning II/ML Assignment/Absenteeism_at_work_classification/Absenteeism_at_work_classification_training.csv",sep = ";")
ncol(abs_trainset_df)

abs_testset_df = read.csv(file = file.path("C:/Users/sandr/Desktop/Machine Learning II/ML Assignment/Absenteeism_at_work_classification/Absenteeism_at_work_classification_test.csv"),  header = TRUE, dec = ".", sep = ";")
```

## Variable Type
```{r Variables}
data.frame(Variable = names(abs_trainset_df),
           Type = sapply(abs_trainset_df, typeof),
           Min=sapply(abs_trainset_df, min),
           Max=sapply(abs_trainset_df, max),
           First_values = sapply(abs_trainset_df, function(x) paste0(head(x),  collapse = ", ")),
           row.names = NULL) %>% kable()
```

The dataframe contains **22** variables.The target variable is **Absenteeism**, a logical variable setting categorizing whether an individual is going to be absent more than 5 hours in total.

## Data overview
Lets look at the statistical summary and histogram of each variable. 

```{r Training Dataset Overview, echo=FALSE}
skim(abs_trainset_df)
```

# Data Cleaning and Preparation
From the data overview we can identify there target variable is sufficiently balanced in training set with 63% False and 47% True. We also dont have any missing values.
However we do have variable which are codified as numbers which are supposed to be categorical and possible outliers in few variables such as age and transportation cost.

## Changing Data Type
We can see from the attribute information (https://archive.ics.uci.edu/ml/datasets/Absenteeism+at+work) that 11 categorical variables have been codified as number in our dataframe. These variables are: **ID**, **ID.Worker**, **Reason.for.absence**, **Month.of.absence**, **Day.of.the.week**, **Seasons**, **Disciplinary.failure**, **Education**, **Social.drinker**, **Social.smoker**,**Absenteeism** 
Before converting these these variables need to combine test dataset with train dataset, and apply the conversion on both. 

```{r Combine Dataset, echo=FALSE}
abs_testset_df$Absenteeism <- 0  #add taget variable Absenteeism into the test dataset
abs_dataset <- rbind(abs_trainset_df, abs_testset_df)
str(abs_dataset) #check dataset
```
We can now convert all the categorical variables to factors. 
```{r Converting Variables, echo=FALSE}
categorical_variables <- c('ID', 'ID.Worker','Education','Reason.for.absence', 'Month.of.absence', 'Day.of.the.week', 'Seasons', 'Disciplinary.failure', 'Social.drinker', 'Social.smoker', 'Absenteeism')
abs_dataset[categorical_variables] <- lapply(abs_dataset[categorical_variables], factor) 

data.frame(Variable = names(abs_dataset),
           Class = sapply(abs_dataset, class),
           First_values = sapply(abs_dataset, function(x) paste0(head(x),  collapse = ", ")),
           row.names = NULL) %>% kable()
```

To make attributes more discriptive we will add the actual category names of variables **Education**, **Seasons**, and **Reason for Absence**. 
```{r Making Categorical Variables Discriptive}
abs_dataset$Reason.for.absence <- recode(abs_dataset$Reason.for.absence, 
                                     '0'='Infectious parasitic diseases',
                                     '1'='Neoplasms',
                                     '2'='Diseases of the blood',
                                     '3'='Endocrine and metabolic diseases',
                                     '4'='Mental and behavioural disorders',
                                     '5'='Diseases of the nervous system',
                                     '6'='Diseases of the eye and adnexa',
                                     '7'='Diseases of the ear and mastoid process',
                                     '8'='Diseases of the circulatory system',
                                     '9'='Diseases of the respiratory system',
                                     '10'='Diseases of the digestive system',
                                     '11'='Diseases of the skin and subcutaneous tissue',
                                     '12'='Diseases of the musculoskeletal system and connective tissue', 
                                     '13'='Diseases of the genitourinary system',
                                     '14'='Pregnancy, childbirth and the puerperium',
                                     '15'='Certain conditions originating in the perinatal',
                                     '16'='Congenital malformations, deformations and chromosomal abnormalities',
                                     '17'='Symptoms, signs and abnormal clinical  findings',
                                     '18'='Injury, poisoning and certain other consequences of external causes',
                                     '19'='causes of morbidity and mortality',
                                     '21'='Factors influencing health status and contact with health services',
                                     '22'='patient follow-up',
                                     '23'='medical consultation',
                                     '24'='blood donation',
                                     '25'='laboratory examination',
                                     '26'='unjustified absence',
                                     '27'='physiotherapy',
                                     '28'='dental consultation')

abs_dataset$Seasons =recode(abs_dataset$Seasons,'1'='summer','2'='autumn','3'='winter','4'='spring')

abs_dataset$Education =recode(abs_dataset$Education, '1'='highschool','2'='graduate','3'='postgraduate','4'='master&PhD')

#Check dataset again
str(abs_dataset$Seasons)
str(abs_dataset$Education)
str(abs_dataset$Reason.for.absence)
str(abs_dataset$Absenteeism)
```
The target variable already has One Hot Encoding and we dont need to make further changes to it. 

## Check for Duplicate Observations
Our unique identifier in the dataset is the **ID** variable and we can use that to see if we have variables that have been duplicated. We do this for just the training part of the data set which is the first 593 records.
```{r Duplicates}
print(paste0("The total number of duplicates in our dataframe is: ", sum(duplicated(abs_dataset$ID[1:593]))))
```

## Outlier Detection

We need to further prepare out dataset by checking for outliers in the numerical variables and identify if it is an error or an actual observation and decide how to move forward with these observations. 
Eventhough the histogram above gives indication of which attributes might contain outliers, 1e will plot all the numerical variables we have in our dataset to make sure and see their distribute better. 
Again we only plot the dataset for training part of out dataset.

**NOTE: The training and test is not yet removed as we still need to explore correlation and decide if we are going to keep all our attributes or remove some of them. **
```{r Boxplot I, fig.width=8}
par(mfrow=c(1,5))
boxplot(abs_dataset$Transportation.expense[1:593],
main = "Transp. Expense",
xlab = "Cost",
ylab = "Amount",
col = "darkcyan",
border = "darkcyan",
horizontal = FALSE,
notch = TRUE
)
boxplot(abs_dataset$Distance.from.Residence.to.Work[1:593],
main = "Dist. Resi.-Work",
xlab = "Distance",
ylab = "KMs",
col = "darkcyan",
border = "darkcyan",
horizontal = FALSE,
notch = TRUE
)
boxplot(abs_dataset$Service.time[1:593],
main = "Service Time",
xlab = "Serv. Time",
ylab = "Years",
col = "darkcyan",
border = "darkcyan",
horizontal = FALSE,
notch = TRUE
)
boxplot(abs_dataset$Age[1:593],
main = "Age",
xlab = "Age",
ylab = "Years",
col = "darkcyan",
border = "darkcyan",
horizontal = FALSE,
notch = TRUE
)
boxplot(abs_dataset$Hit.target[1:593],
main = "Hit.target",
xlab = "Goals",
ylab = "% Achievment",
col = "darkcyan",
border = "darkcyan",
horizontal = FALSE,
notch = TRUE
)
```

```{r Boxplots II, fig.width=9}
par(mfrow=c(1,6))

boxplot(abs_dataset$Work.load.Average.day[1:593],
main = "Work Load",
xlab = "Work Load",
ylab = "Avg. Workload",
col = "darkcyan",
border = "darkcyan",
horizontal = FALSE,
notch = TRUE
)
boxplot(abs_dataset$Weight[1:593],
main = "Weight",
xlab = "Weight",
ylab = "Kgs",
col = "darkcyan",
border = "darkcyan",
horizontal = FALSE,
notch = TRUE
)
boxplot(abs_dataset$Height[1:593],
main = "Height",
xlab = "Height",
ylab = "Meters",
col = "darkcyan",
border = "darkcyan",
horizontal = FALSE,
notch = TRUE
)
boxplot(abs_dataset$Body.mass.index[1:593],
main = "BMI",
xlab = "BMI",
ylab = "BMI",
col = "darkcyan",
border = "darkcyan",
horizontal = FALSE,
notch = TRUE
)
boxplot(abs_dataset$Son[1:593],
main = "# of Childred",
xlab = "# of Childred",
ylab = "Child",
col = "darkcyan",
border = "darkcyan",
horizontal = FALSE,
notch = TRUE
)
boxplot(abs_dataset$Pet[1:593],
main = "# of Pet",
xlab = "# of Pet",
ylab = "Pet",
col = "darkcyan",
border = "darkcyan",
horizontal = FALSE,
notch = TRUE
)
```

## Treating Outliers

The initial 5 boxplots show that We have a few outliers in Transport expense, service time, age, and hit.targe. Additionally there are outliers detected in work Load, height and number of pets, in the second group of boxplots. 

Refering back to the overview where we see the minimum and maximum value of these variables, we can establish that there is no error in these data points and are actual observations which are deviations from the range in our dataset.

Considering our obejective is to predict if an employee is going to be absent from a workplace, we want to be able to predict this is as many employees as possible. So if a value that lies outisde the range of other values in the training is justfied, the deviation is not extreme and we can expect smiliar deviations from the data outside this set, we can retain the data. 

So for outlier detected in **transport expense** , we can keep this outlier as it is not extreme deviation, we expect our real world data to deviate from the range and want our model to be able to predict for such occurances. 

Similarly, we will keep outliers in the **percentage of achievement of goals in the (Hit.Target)** and **work load**, as we expect to have employees that might be performing less that others or have higher work load than the rest. We want our model to predict for such kinds of employees as well. 

In the case of the outliers detected in the **number of service time and age**, we can assume that there will be very limited employees that will have higher values than the range in the training set. This might be only employees that are in management position which have stayed with company for long time and we don´t expect such employees in out real world data, thus we don´t want our model to train for such unique cases. **However, we will not remove these outliers in number of service time and age, but instead will bin this variables and convert them to categorical variables as in we don't necessarly need out model to train for each unique case and we assume the category group with have simiar manner.**

For outliers detected in Height, we can keep this records as they are for now as we see that there are no outliers in BMI which combines weight and height (The formula for BMI is weight in kilograms divided by height in meters squared). Accordingly expect high correlation between these values and there is not need for our model to train on weight and height if it uses BMI. And so our model will not be taking into consideration these deviated values from height. 

We have three outliers in number of pets owned. Eventhough it is not necessarly an error and employees can have as much as 5 to 8 pets we don't want our distribution to be affected by such values. We want out model to only capture wether having a pet or not having a pet affects absentieseem and this further justified by the distribution of those who don't have pets and those which do.


```{r OneHot Encode  - Number of Pets}
abs_dataset$Pet_encoded<-abs_dataset$Pet
abs_dataset$Pet_encoded[abs_dataset$Pet_encoded > 0] <- 1

abs_dataset[5:15,c('Pet','Pet_encoded')]

```

## Advanced Factorization
Following the handeling out outliers, further factorization through binning can be done on variables which have levels of detail that we don't need our model to train on and capture. In this dataset we will bin the variables **Age**, **Years of service**, **distance from residence to work**, and **transport cost**.

To check that our binning is actually improving our model prediction we will keep the original numeric variable and we will create a new factor variable for each of these attributes. 

- **Age**
We can bin our employees into 
    - 18 to 25 : very young employees
    - 26 to 35 : young employees
    - 36 to 45 : middle age employees
    - Above 45 : old age employess
```{r Binning Age - Bucketize}
abs_dataset$age_bin <-.bincode(abs_dataset$Age, c(18, 25, 35,45,100))
abs_dataset$age_bin<- recode(abs_dataset$age_bin,`1`="Very Young Employee",`2`="Young Employee",`3`="Middle Age Employee",`4`="Old Age Employee")
abs_dataset[5:15,c('Age','age_bin')]
```


- **Service Time **
We can bin our employees into  4 categories based on the quantiles, which will create bins automatically using the dataset distribution.
```{r Binning Service time - Bucketize}
service_time_bins<-bins.quantiles(abs_dataset$Service.time, 4, 4, verbose = FALSE)
service_time_bins$binc
#[1, 9] [10, 13] [14, 16] [17, 29] we will use this range to create binned variable.

abs_dataset$service.time_bin <-.bincode(abs_dataset$Service.time, c(0, 10, 14,17,30),FALSE, TRUE)
abs_dataset$service.time_bin<- recode(abs_dataset$service.time_bin,`1`="1-9 yrs",`2`="10 to 13 yrs",`3`="14 to 16 yrs",`4`="17 to 29 yrs")
abs_dataset[100:105,c('Service.time','service.time_bin')] #viewing binned data
```


- **Distance from residence to work**, and **Transportation Expense**
The two variables will be binned as follows:
  Transportation Expense:
    - 100 to 199 : Cheap
    - 200 to 299 : Normal
    - Above 300: Expensive
  
```{r Binning Transportation Expense - Bucketize}
abs_dataset$Transportation.expense_bin <-.bincode(abs_dataset$Transportation.expense, c(100, 200, 300,2000), FALSE, TRUE)
abs_dataset$Transportation.expense_bin<- recode(abs_dataset$Transportation.expense_bin, `1`="Cheap",`2`="Normal",`3`="Very Expensive")
abs_dataset[210:215,c('Transportation.expense','Transportation.expense_bin')]

```
  
Distance from residence to work : Binned as "Close", "Far" and "Very Far" through bin generated by quantile distribution. 
```{r Binning Distance from Residence to Work}

distance_bins<-bins.quantiles(abs_dataset$Distance.from.Residence.to.Work, 4, 4, verbose = FALSE)
distance_bins$binc
#[5, 16] [17, 26] [27, 50] [51, 52]  we will use this range to create binned variable.
abs_dataset$Distance.Resi.to.Work_bin <-.bincode(abs_dataset$Distance.from.Residence.to.Work, c(5, 17, 27,51, 53), FALSE, TRUE)
abs_dataset$Distance.Resi.to.Work_bin <- recode(abs_dataset$Distance.Resi.to.Work_bin,`1`="Close",`2`="Average Distance",`3`="Far",`4`="Very Far")
abs_dataset[100:105,c('Distance.from.Residence.to.Work','Distance.Resi.to.Work_bin')] #viewing binned data

```
## Normalization
The variable daily average workload could be 
Algorithms that work with distances tend to work better after normalization, but this doesn't guaranteer better performance so we will keep the original variable to check difference.

```{r Normalizing Data, fig.width=9, fig.height=3}

bar_wl_month<-ggplot(abs_dataset, aes(x = abs_dataset$Work.load.Average.day, y = abs_dataset$Month.of.absence), group = abs_dataset$Absenteeism, color = abs_dataset$Absenteeism) +
geom_line()
bar_wl_abs<-ggplot(abs_dataset, aes(x = abs_dataset$Absenteeism, y = abs_dataset$Work.load.Average.day, fill=Absenteeism)) +
geom_bar(stat = "summary", fun.y= "mean")
bar_wl_dow<-ggplot(abs_dataset, aes(x = abs_dataset$Day.of.the.week, y = abs_dataset$Work.load.Average.day, fill=Day.of.the.week)) +
geom_bar(stat = "summary", fun.y= "mean")

gridExtra::grid.arrange(bar_wl_abs, bar_wl_month, bar_wl_dow,ncol=3)

```
the plot above compares how work load relates to variables such as month of absence and what the average work load for those that were absent and were not. There seems to be no clear correlation as the average workload for those with long absentees and those with not is similar. And there is no trend in month of absense and average workload 



## Exploring Other Factor Variables

```{r}
bar1 = ggplot(data = abs_dataset, aes(x = ID)) + geom_bar() + ggtitle("Count of ID") + theme_bw()
bar2 = ggplot(data = abs_dataset, aes(x = Reason.for.absence)) + geom_bar() + 
  ggtitle("Count of Reason for absence") + theme_bw()
bar3 = ggplot(data = abs_dataset, aes(x = Month.of.absence)) + geom_bar() + ggtitle("Count of Month") + theme_bw()
bar4 = ggplot(data = abs_dataset, aes(x = Disciplinary.failure)) + geom_bar() + 
  ggtitle("Count of Disciplinary failure") + theme_bw()
bar5 = ggplot(data = abs_dataset, aes(x = Education)) + geom_bar() + ggtitle("Count of Education") + theme_bw()
bar6 = ggplot(data = abs_dataset, aes(x = Number_Children)) + geom_bar() + ggtitle("Number of Children") + theme_bw()
bar7 = ggplot(data = abs_dataset, aes(x = Social.smoker)) + geom_bar() + 
  ggtitle("Count of Social smoker") + theme_bw()

gridExtra::grid.arrange(bar1,bar2,bar3,bar4,ncol=2)
gridExtra::grid.arrange(bar5,bar6,bar7,ncol=2)
```



## Correlation Between Exlanatory Variables

```{r Correlation Test - BMI with Weight and Height}
cor.test(abs_dataset$Height, abs_dataset$Body.mass.index, method="pearson")
cor.test(abs_dataset$Weight, abs_dataset$Body.mass.index, method="pearson")
```
The correlation test above using the Pearson method for both **Weight** and **Height** with **Body.mass.index** as expected resulted in values very less than the significance level alpha = 0.05.

```{r Correlation Test - Transport Expense and Distance}
cor.test(abs_dataset$Transportation.expense, abs_dataset$Distance.from.Residence.to.Work, method="pearson")
```

## Continued Data Clearning

```{r More Data Cleaning}
colnames(abs_dataset)[which(names(abs_dataset) == "Son")] <- "Number_Children"
#Renaming "Son" Attribute to more accurate name

data.frame(Variable = names(abs_dataset),
           Class = sapply(abs_dataset, class),
           First_values = sapply(abs_dataset, function(x) paste0(head(x),  collapse = ", ")),
           row.names = NULL) %>% kable()
```



## Train and Test Split
```{r Split Train and Test - Inital}
training <- abs_dataset[1:593,]  
test <- abs_dataset[594:740,]

```

# Feature Engineering



## Baseline Model
We will fit a general linear model that will be used to compare our model before and after data cleaning and feature filtering.

```{r}

```




